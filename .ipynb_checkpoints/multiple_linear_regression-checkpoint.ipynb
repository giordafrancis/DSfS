{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import dot, Vector\n",
    "\n",
    "def predict(x: Vector, theta: Vector) -> float:\n",
    "    \"\"\"assumes that first element of x is 1\"\"\"\n",
    "    return dot(x, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further assumptions of the Least Squares Model:\n",
    "\n",
    "- the first is that features of vector X are linearly independent;meaning there is no way to write any one as a weighted sum of some of the others. It this assumtion fails it's is impossible to correctly estimate theta\n",
    "\n",
    "- the second assumption is that the features of X are all uncorrelated with the errors E. If this fals to be the case, our estimate theta will systematiclly be incorrect\n",
    "\n",
    "pag 191 to 193 have more details on this. Also more detail in this [article](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from linear_algebra import Vector\n",
    "\n",
    "def error(x: Vector, y: float, theta: Vector) -> float:\n",
    "    return predict(x, theta) - y\n",
    "\n",
    "def squared_error(x: Vector, y: float, theta: Vector) -> float:\n",
    "    return error(x, y, theta) ** 2\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "theta = [4, 4, 4] # so prediction = 4 + 8 + 12 = 24\n",
    "\n",
    "assert error(x, y, theta) == -6\n",
    "assert squared_error(x, y, theta) == 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradient(x: Vector, y: float, theta: Vector) -> Vector:\n",
    "    err = error(x, y, theta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x, y, theta) == [-12, -24, -36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gradient descent we can know compute the optimal theta. first lets write a least_squares_fit function that can work with any dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from linear_algebra import vector_mean\n",
    "from gradient_descent import gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(xs: List[Vector], ys: List[float], learning_rate: float = 0.001, num_steps: int = 1000, batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the theta that minimizes the sum of squared errors assuming the model y = dot(x, theta)\n",
    "    \"\"\"\n",
    "    # start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    for epoch in range(num_steps):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, - learning_rate)\n",
    "        print(f'epoch is {epoch}; current guess is {guess}')\n",
    "    return guess  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import daily_minutes_good\n",
    "from inputs import inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "theta = least_squares_fit(inputs, daily_minutes_good, learning_rate, 5000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minutes= 30.58 + 0.972 friends -1.87 work hours + 0.923 phd\n",
    "assert 30.50 < theta[0] < 30.70\n",
    "assert 0.96 < theta[1] < 1.00 \n",
    "assert -1.89 < theta[2] < -1.85\n",
    "assert 0.91 < theta[3] < 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think of the coefficients of the model as representing all-else-being-equalestimates  of  the  impacts  of  each  factor.  All  else  being  equal,  each  additional  friendcorresponds to an extra minute spent on the site each day. All else being equal, eachadditional hour in a user’s workday corresponds to about two fewer minutes spent onthe  site  each  day.  All  else  being  equal,  having  a  PhD  is  associated  with  spending  an extra minute on the site each day.\n",
    "\n",
    "What this doesnt capture is interactions between features. It's possible works hours effect is sifferent with people with many friends. One way to handle this is to introduce a new variable with the product of friends and work hours. \n",
    "\n",
    "Or  it’s  possible  that  the  more  friends  you  have,  the  more  time  you  spend  on  the  siteup  to  a  point,  after  which  further  friends  cause  you  to  spend  less  time  on  the  site.(Perhaps  with  too  many  friends  the  experience  is  just  too  overwhelming?)  We  couldtry  to  capture  this  in  our  model  by  adding  another  variable  that’s  the  square  of  thenumber of friends.\n",
    "\n",
    "Once we start adding varaibles we need to worry about weather their coefficients matter. There are no limits to the numbers of products, logs, squares and high powers that can be added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_linear_regression import total_sum_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_r_squared(xs: List[Vector], ys:Vector, theta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, theta) ** 2\n",
    "                                for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_squares(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, theta) < 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared tends to increase the more varables are added to the model. Because of this, in a multiple regression, we also need to look at the standard errors ofthe  coefficients,  which  measure  how  certain  we  are  about  our  estimates  of  each  theta_i.\n",
    "The regression as a whole may fit our data very well, but if some of the independentvariables are correlated (or irrelevant), their coefficients might not mean much.The typical approach to measuring these errors starts with another assumption—that the errors **εi** are independent normal random variables with mean 0 and some shared(unknown) standard deviation σ. In that case, we (or, more likely, our statistical soft‐ware) can use some linear algebra to find the standard error of each coefficient. Thelarger it is, the less sure our model is about that coefficient. Unfortunately, we’re notset up to do that kind of linear algebra from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
