{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently when doing data science, we’ll be trying to the find the best model for a\n",
    "certain situation. And usually “best” will mean something like “minimizes the error\n",
    "of the model” or “maximizes the likelihood of the data.” In other words, it will represent\n",
    "the solution to some sort of optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For functions like ours, the gradient (this is the vector\n",
    "of partial derivatives) gives the input direction in which the function most quickly\n",
    "increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach is to maximizing the above function is to\n",
    "\n",
    "- pick a random starting point, \n",
    "- compute the gradient, \n",
    "- take a small step in the gradient direction and repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the Gradient and know as gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the function below with one variable. It's derivative at a point x measures how f(x) changes when a make a small change to f(x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\" Computes the sum of squared elements in v \"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is defined as the slope pf the tangent line at (x, f(x)), while the difference quotiente is the slope of the not-quite-tangent line that runs through (x+h, f(x+h)). \n",
    "\n",
    "As the step h gets smaller and smaller, the not so quite tangent line gets coloser and closer. Defining below the not so tangent line approximation of the derivative of f(x).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f: Callable[[float], float], x: float, h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many functions is easy to claculate the derivates. Example below for the derivate of the square function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets estimate the derivatives by evaluating the difference quotient for a small step e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(-10, 11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.001)\n",
    "            for x in xs]\n",
    "# plot to show they are basically the same value\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Actual square derivative vs estimates\")\n",
    "plt.plot(xs, actuals, 'rx', label = 'Actuals')\n",
    "plt.plot(xs, estimates, 'b+', label = 'Estimates')\n",
    "plt.legend(loc=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When f is a function of 2 or more variables, it has many *partial derivatives*; each indicating how f changes when we make small changes in just one of the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float], v: Vector, i: int, h: float) -> float:\n",
    "    \"\"\" Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j==i else 0) # add h to just the ith element of v\n",
    "        for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) /h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float], v: Vector, h: float = 0.0001) -> Vector: \n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "           for i in range(len(v))]                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is computtatinly expensive, as per each feature we woudl have to compute 2n operations to calculate the tangent line that approximates to the partial derivative. Math derivatives to be used beyond this point. Sum of squares gradient as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From intuition the function sum_of_squares will be at it's minumum value if the input is a vector of zeroes for all features. \n",
    "Let's prove the below based on the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from linear_algebra import distance, add, scalar_multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Updates the theta parameter after one epoch\n",
    "     Moves step_size in the gradient direction from 'v'\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random staring point\n",
    "\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "last_5_epochs = deque([],5) # using deque to store last 5, not used\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v) # compute the gradient at v\n",
    "    v = gradient_step(v, grad, -0.01) # take a negative learning rate, to optmize minimum\n",
    "    print(epoch, v)\n",
    "    last_5_epochs.append(v)\n",
    "#print(last_5_epochs)\n",
    "\n",
    "assert distance(v, [0, 0, 0]) < 0.001 # v should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gradient Descent to fit models\n",
    "\n",
    "- we will have some data set \n",
    "- and some hypothesized function for the data depending or one or more features\n",
    "- we will also have a loss function that measures how well the model fits our data\n",
    "\n",
    "If you assume your data as being fixed; then your loss function tells us how good or bad any particular model parameters are. \n",
    "We will use the gradient descent to find th eparamters that minimize the loss function:\n",
    "\n",
    "Let's start with an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from -50 to 49 and y is always 20 * x + 5\n",
    "inputs = [(x, 20 *x + 5) for x in range(-50, 50)]\n",
    "\n",
    "# the function below determines the gradient based on the error from a single data point\n",
    "\n",
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept # the single point prediction of the model\n",
    "    error = (predicted - y) # error is (predicted - actual)\n",
    "    squared_error = error ** 2 # we will minimize the square error\n",
    "    grad = [ 2 * error * x,  2 * error] # using this gradient; \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the above computation was for a single data point. For the whole dataset we'll look at the mean squared error. And the gradient of the mean squared error is just the mean of the individual gradients. \n",
    "\n",
    "we're going to :\n",
    "\n",
    "- start with a random value for theta\n",
    "- compare the mean of the gradients\n",
    "- adjust theta in that direction\n",
    "- Repeat\n",
    "\n",
    "After several epochs (each pass through the dataset) we shoudl learn something approxing the correct parameters. Remmeber we know the correct parameters of theta for comparison. In the below the algorithm will learn from y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import vector_mean\n",
    "\n",
    "# Start with random values for slope and intercept\n",
    "# remmeber we are aiming at a slope around 20 and intercept at 5\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)] \n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) \n",
    "                        for x, y in inputs])  # here grad passes all data points (x,y) -> one batch pass per epoch\n",
    "                                              # returns 1 x 2 vector gradient\n",
    "    \n",
    "    # take a step in that direction\n",
    "    theta = gradient_step(theta, grad, - learning_rate)  # updates theta parameters\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.99 < slope < 20.01\n",
    "assert 4.99 < intercept < 5.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Setback of this type of the batch gradient descent is we need to evaluate the whole dataset at the grad step prior to take a gradient step\n",
    "this would be prohibitive with larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minibatch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch gradient descent technique we compute the gradient and take a gradient step based on a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T') # this allows us to type generic functions more in the book pag 108\n",
    "\n",
    "def minibatches(dataset : List[T], batch_size: int, shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates 'batch_size' minibatches from the dataset\"\"\"\n",
    "    # start indexes 0, batch_size, 2 * batch_size...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    if shuffle: random.shuffle(batch_starts) # shuffle the batches\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        #print(f\"{start}:{end}\") # assist in viewing the minibatches indexes\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can solve the same problem wth the minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
    "\n",
    "for epoch in range(1000): # for each epoch we update theta len(datase)/ batch_size; 5 in this case\n",
    "    for batch in minibatches(inputs, batch_size = 20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta)\n",
    "                           for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1 , \"slope should be around 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be around 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variation is stochastic gradient descent, where gradient steps are taken based on one training example at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [19.88783310521385, 4.6506617683677645]\n",
      "1 [19.979577854836535, 4.936398154718088]\n",
      "2 [19.996281871739363, 4.988420422218826]\n",
      "3 [19.999323064367015, 4.9978917809539025]\n",
      "4 [19.999876754695077, 4.9996161701548845]\n",
      "5 [19.999977561522172, 4.999930118575574]\n",
      "6 [19.999995914771024, 4.999987277139732]\n",
      "7 [19.999999256228705, 4.99999768363088]\n",
      "8 [19.999999864586357, 4.99999957827361]\n",
      "9 [19.999999975346114, 4.9999999232189545]\n",
      "10 [19.999999995511402, 4.999999986020961]\n",
      "11 [19.9999999991828, 4.999999997454921]\n",
      "12 [19.99999999985123, 4.999999999536634]\n",
      "13 [19.99999999997291, 4.999999999915675]\n",
      "14 [19.99999999999509, 4.99999999998463]\n",
      "15 [19.99999999999911, 4.999999999997218]\n",
      "16 [19.999999999999872, 4.9999999999995115]\n",
      "17 [19.99999999999996, 4.999999999999889]\n",
      "18 [20.0, 4.999999999999983]\n",
      "19 [20.0, 4.999999999999983]\n",
      "20 [20.0, 4.999999999999983]\n",
      "21 [20.0, 4.999999999999983]\n",
      "22 [20.0, 4.999999999999983]\n",
      "23 [20.0, 4.999999999999983]\n",
      "24 [20.0, 4.999999999999983]\n",
      "25 [20.0, 4.999999999999983]\n",
      "26 [20.0, 4.999999999999983]\n",
      "27 [20.0, 4.999999999999983]\n",
      "28 [20.0, 4.999999999999983]\n",
      "29 [20.0, 4.999999999999983]\n",
      "30 [20.0, 4.999999999999983]\n",
      "31 [20.0, 4.999999999999983]\n",
      "32 [20.0, 4.999999999999983]\n",
      "33 [20.0, 4.999999999999983]\n",
      "34 [20.0, 4.999999999999983]\n",
      "35 [20.0, 4.999999999999983]\n",
      "36 [20.0, 4.999999999999983]\n",
      "37 [20.0, 4.999999999999983]\n",
      "38 [20.0, 4.999999999999983]\n",
      "39 [20.0, 4.999999999999983]\n",
      "40 [20.0, 4.999999999999983]\n",
      "41 [20.0, 4.999999999999983]\n",
      "42 [20.0, 4.999999999999983]\n",
      "43 [20.0, 4.999999999999983]\n",
      "44 [20.0, 4.999999999999983]\n",
      "45 [20.0, 4.999999999999983]\n",
      "46 [20.0, 4.999999999999983]\n",
      "47 [20.0, 4.999999999999983]\n",
      "48 [20.0, 4.999999999999983]\n",
      "49 [20.0, 4.999999999999983]\n",
      "50 [20.0, 4.999999999999983]\n",
      "51 [20.0, 4.999999999999983]\n",
      "52 [20.0, 4.999999999999983]\n",
      "53 [20.0, 4.999999999999983]\n",
      "54 [20.0, 4.999999999999983]\n",
      "55 [20.0, 4.999999999999983]\n",
      "56 [20.0, 4.999999999999983]\n",
      "57 [20.0, 4.999999999999983]\n",
      "58 [20.0, 4.999999999999983]\n",
      "59 [20.0, 4.999999999999983]\n",
      "60 [20.0, 4.999999999999983]\n",
      "61 [20.0, 4.999999999999983]\n",
      "62 [20.0, 4.999999999999983]\n",
      "63 [20.0, 4.999999999999983]\n",
      "64 [20.0, 4.999999999999983]\n",
      "65 [20.0, 4.999999999999983]\n",
      "66 [20.0, 4.999999999999983]\n",
      "67 [20.0, 4.999999999999983]\n",
      "68 [20.0, 4.999999999999983]\n",
      "69 [20.0, 4.999999999999983]\n",
      "70 [20.0, 4.999999999999983]\n",
      "71 [20.0, 4.999999999999983]\n",
      "72 [20.0, 4.999999999999983]\n",
      "73 [20.0, 4.999999999999983]\n",
      "74 [20.0, 4.999999999999983]\n",
      "75 [20.0, 4.999999999999983]\n",
      "76 [20.0, 4.999999999999983]\n",
      "77 [20.0, 4.999999999999983]\n",
      "78 [20.0, 4.999999999999983]\n",
      "79 [20.0, 4.999999999999983]\n",
      "80 [20.0, 4.999999999999983]\n",
      "81 [20.0, 4.999999999999983]\n",
      "82 [20.0, 4.999999999999983]\n",
      "83 [20.0, 4.999999999999983]\n",
      "84 [20.0, 4.999999999999983]\n",
      "85 [20.0, 4.999999999999983]\n",
      "86 [20.0, 4.999999999999983]\n",
      "87 [20.0, 4.999999999999983]\n",
      "88 [20.0, 4.999999999999983]\n",
      "89 [20.0, 4.999999999999983]\n",
      "90 [20.0, 4.999999999999983]\n",
      "91 [20.0, 4.999999999999983]\n",
      "92 [20.0, 4.999999999999983]\n",
      "93 [20.0, 4.999999999999983]\n",
      "94 [20.0, 4.999999999999983]\n",
      "95 [20.0, 4.999999999999983]\n",
      "96 [20.0, 4.999999999999983]\n",
      "97 [20.0, 4.999999999999983]\n",
      "98 [20.0, 4.999999999999983]\n",
      "99 [20.0, 4.999999999999983]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "random.shuffle(inputs) #not on book; input observations need to be shuffled\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1 , \"slope should be around 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be around 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
