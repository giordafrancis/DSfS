{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are perhaps one of the most popular and talked about machine learning\n",
    "algorithms. They were extremely popular around the time they were developed in the 1990s\n",
    "and continue to be the go-to method for a high-performing algorithm with little tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal-Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximal-margin classifier is a hypothetical classifier that best explains how the SVM works in practice. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two dimensions you can visualize this as a line and let's assume that all of our input points can be completely separated by this line. For example:\n",
    "\n",
    "    theta_0 + (theta_1 x X1) + (theta_2 X X2) = 0\n",
    "\n",
    "Where theta_1 and theta_2 are the coefficients that determine the slope of the line and the intercept (theta_0) are found by the learning algorithm. By plugging input vector into the line equation, you can calculate weather a new point is above or below the line. \n",
    "\n",
    "- Above the line the equation returns a value greater then 0, and the point belongs to class 0\n",
    "- Below the line the equation returns a value smaller then 0, and the point belongs to class 1\n",
    "- a value really close to the line will be close to 0 and therefore might be difficult to classify\n",
    "- on the other hand a larger magnitude value will be more easy for the model to classify.\n",
    "\n",
    "The distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that has the largest margin. **The margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice real data is messy and cannot be separated perfectly with a hyperplane. The constraint of maximizing the margin of the line that separates the classes must be relaxed. This is often called the soft margin classifier. This change allows some points in the training data to violate the separating line.  An additional set of coefficients are introduced that give the margin\n",
    "wiggle room in each dimension. These coefficients are sometimes called slack variables. This\n",
    "increases the complexity of the model as there are more parameters for the model to fit to the\n",
    "data to provide this complexity. A tuning parameter is introduced called simply C that defines the magnitude of the wiggle\n",
    "allowed across all dimensions.\n",
    "\n",
    "- the smaller the value of C, the more sensitive the algorithm is to the training data (higher variance and lower bias)\n",
    "- the larger the value of C, the less sensitive the algorithm is to the training data (lower variance and higher bias) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (Kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM algorithm is implemented in practice using a kernel. A powerful insight is that the liner SVM can be rephrased using the inner product on any given two observations, rather then the observations themselves. The equation for making the prediction for a new input using the dot product of vector x and each **support vector** (xi) is calculated below:\n",
    "\n",
    "    f(x) = B0 + sum(theta_i * dot(x, xi))\n",
    "\n",
    "The inner product involves calculating the inner products of a new input vector (x) with **all** the support vectors in training data. The coefficients B0 (bias) and theta_i (for each input) must be estimated from the training data by the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liner kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot-product is called the kernel and can be re-written as: \n",
    "\n",
    "    K(x, xi) = dot(x, xi)\n",
    "    \n",
    "The kernel defines the similarity or a distance measure between new data and the support\n",
    "vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because\n",
    "the distance is a linear combination of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the dot-product, we can use a polynomial kernel:\n",
    "\n",
    "    K(x, xi) = 1 + dot(x, xi) ** d\n",
    "    \n",
    "Where the degree of the polynomial must be specified by hand to the learning algorithm.\n",
    "When d = 1 this is the same as the linear kernel. The polynomial kernel allows for curved lines\n",
    "in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a more complex radial kernel. For example:\n",
    "\n",
    "    K(x, xi) = e ** -gamma * sum((x - xi) ** 2)\n",
    "    \n",
    "Where gamma is a parameter that must be specified to the learning algorithm. A good\n",
    "default value for gamma is 0.1, where gamma is often 0 < *gamma* < 1. The radial kernel is\n",
    "very local and can create complex regions within the feature space, like closed polygons in a\n",
    "two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above support vector classifiers can be written as a dot product. The Kernel trick is to replace the dot product with a Kernel:\n",
    "\n",
    "    f(x) = B0 + sum(theta_i * K(x, xi))\n",
    "    \n",
    "Allowing for non-linear decision boundaries and computational efficiencies. The coefficients B0 (bias) and theta_i  must be estimated from the training data by the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to learn a SVM Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model needs to be solved using an optimization procedure. If implementing\n",
    "the algorithm as an exercise, you could use a variation of gradient descent called sub-gradient\n",
    "descent.\n",
    "\n",
    "There are specialized optimization procedures that re-formulate the optimization problem\n",
    "to be a Quadratic Programming problem. The most popular method for fitting SVM is the\n",
    "Sequential Minimal Optimization (SMO) method that is very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data for SVM\n",
    "\n",
    "This section lists some suggestions for how to best prepare your training data when learning an\n",
    "SVM model.\n",
    "\n",
    "- Numerical Inputs: SVM assumes that your inputs are numeric. If you have categorical\n",
    "inputs you may need to covert them to binary dummy variables (one variable for each\n",
    "category).\n",
    "- Binary Classification: Basic SVM as described in this chapter is intended for binary\n",
    "(two-class) classification problems. Although, extensions have been developed for regression\n",
    "and multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- The Maximal-Margin Classifier that provides a simple theoretical model for understanding\n",
    "SVM.\n",
    "- The Soft Margin Classifier which is a modification of the Maximal-Margin Classifier to\n",
    "relax the margin to handle noisy class boundaries in real data.\n",
    "- Support Vector Machines and how the learning algorithm can be reformulated as a\n",
    "dot-product kernel and how other kernels like Polynomial and Radial can be used.\n",
    "- How you can use numerical optimization to learn the hyperplane and that efficient implementations use an alternate optimization scheme called Sequential Minimal Optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
