{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbours is one of the simplest predictive models. It makes no mathematical assumptions and it doesn't required any sort of heavy machinery. the only things it requires are:\n",
    "\n",
    "- Some notion of distance\n",
    "- An assumption that points are close to one another are similar\n",
    "\n",
    "That simple assumption also narrows down the understanding of the hypothesis behind the data. Any new points will be classified based on the distance notion above not what causes the classification.\n",
    "\n",
    "the algorithm trains all labels based on training; one a test set is reviewed it's distance is compared to all training set data then ordered by smallest distance and finally a vote takes place (counter) to ascertain who as the majority of labels within the K smallest distances - that assigns the labelling.\n",
    "\n",
    "\n",
    "from scikit learn doc\n",
    "https://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data \n",
    "\n",
    "Note: assigning the number of K required was not covered. Often is done via  a validation set. \n",
    "\n",
    "The curse of dimensionality was also covered : KNN pitfall as it runs into problems in higher dimensions. The higher the dim the vastest is the space.  If KNN is to be used in higher dimensions consider dimensionality reduction first. \n",
    "\n",
    "Pag 170 to 173 covers the curse of dimensionality with examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def majority_vote(labels: List[str]) -> str:\n",
    "    \"\"\"assumes that labels are ordered from nearest to farthest\"\"\"\n",
    "    vote_counts = Counter(labels)\n",
    "    winner, winner_count = vote_counts.most_common(1)[0]\n",
    "    num_winners = len([count\n",
    "                       for count in vote_counts.values()\n",
    "                       if count == winner_count])\n",
    "    if num_winners == 1:\n",
    "        return winner\n",
    "    else:\n",
    "        return majority_vote(labels[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert majority_vote(['a', 'b', 'c', 'b', 'a']) == 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from linear_algebra import Vector, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledPoint(NamedTuple):\n",
    "    point: Vector\n",
    "    label: str\n",
    "\n",
    "def knn_classify(k: int, labeled_points: List[LabeledPoint], new_point: Vector) -> str:\n",
    "    # Order the labelled points from nearest to farthest\n",
    "    by_distance = sorted(labeled_points, key= lambda lp: distance(lp.point, new_point))\n",
    "    # find the labels for the k closest\n",
    "    k_nearest_labels = [lp.label for lp in by_distance[:k]]\n",
    "    \n",
    "    return majority_vote(k_nearest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: The iris Dataset\n",
    "\n",
    "We will try to build a model that can preditc the class (species) from the firsts four measurements. Our nearest neighbors function expects a LabeledPoint so lets represent our data that way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('iris.dat', 'w') as f:\n",
    "    f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1,3.5,1.4,0.2,Iris-setosa\\n4.9,3.0,1.4,0.2,Iris-setosa\\n4.7,3.2,1.3,0.2,Iris-setosa\\n4.6,3.1,1.5,0.2,'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_iris_row(row: List[str]) -> LabeledPoint:\n",
    "    \"\"\" sepal_length, sepal_width, petal_length, petal_width, class\n",
    "    \"\"\"\n",
    "    if row:\n",
    "        measurements = [float(value) for value in row[:-1]]\n",
    "        # print(row)\n",
    "        # class is e.g. \"Iris-virginica\"; we just want virginica\n",
    "        label = row[-1].split(\"-\")[-1]\n",
    "        return LabeledPoint(measurements, label)\n",
    "\n",
    "with open('iris.dat') as f:\n",
    "    reader = csv.reader(f)\n",
    "    iris_data = [parse_iris_row(row) for row in reader if row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(point=[5.1, 3.5, 1.4, 0.2], label='setosa'),\n",
       " LabeledPoint(point=[4.9, 3.0, 1.4, 0.2], label='setosa'),\n",
       " LabeledPoint(point=[4.7, 3.2, 1.3, 0.2], label='setosa')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also group the points by species/ label for ploting\n",
    "\n",
    "points_by_species : Dict[str, List[Vector]] = defaultdict(list)\n",
    "\n",
    "for iris in iris_data:\n",
    "    points_by_species[iris.label].append(iris.point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lattice plot done in the book pag 168; skipped but good example for matplotlib dive in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from machine_learning import split_data\n",
    "\n",
    "random.seed(12)\n",
    "iris_train, iris_test = split_data(iris_data, 0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set will be the neighbors that we will use to classify the points in  the test set.\n",
    "we will need to choose the k \n",
    "in a real aplication a validation set is used to determine this\n",
    "here just use k = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# track how many times we see (predictic, actual)\n",
    "\n",
    "confusion_matrix: Dict[Tuple[str, str], int] = defaultdict(int)\n",
    "defaultdict(int)\n",
    "num_correct = 0\n",
    "\n",
    "for iris in iris_test:\n",
    "    predicted = knn_classify(5, iris_train, iris.point)\n",
    "    actual = iris.label\n",
    "    \n",
    "    confusion_matrix[(predicted, actual)] += 1\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777 defaultdict(<class 'int'>, {('setosa', 'setosa'): 13, ('versicolor', 'versicolor'): 15, ('virginica', 'virginica'): 16, ('virginica', 'versicolor'): 1})\n"
     ]
    }
   ],
   "source": [
    "pct_correct = num_correct/ len(iris_test)\n",
    "print(pct_correct, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris =  LabeledPoint(point=[6.3, 3.3, 4.7, 1.6], label='versicolor')\n",
    "#knn_classify(5, iris_train, iris.point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by_distance = sorted(iris_train, key= lambda lp: distance(lp.point, iris.point))\n",
    "#k_nearest_labels = [lp.label for lp in by_distance[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority_vote(k_nearest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
