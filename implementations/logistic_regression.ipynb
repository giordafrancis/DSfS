{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My attempt to implment logictic regression from scracth. data is from master machine learnng algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[\n",
    "[2.7810836, 2.550537003, 0],\n",
    "[1.465489372, 2.362125076, 0],\n",
    "[3.396561688, 4.400293529, 0],\n",
    "[1.38807019, 1.850220317,0],\n",
    "[3.06407232, 3.005305973, 0],\n",
    "[7.627531214, 2.759262235, 1],\n",
    "[5.332441248, 2.088626775, 1],\n",
    "[6.922596716, 1.77106367, 1],\n",
    "[8.675418651, -0.242068655, 1],\n",
    "[7.673756466, 3.508563011, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent_ols import Vector\n",
    "from Multivariate_linear_regression import dot, vector_mean, evaluate_gradient,add_intercept_vectors, predict_point, chunks, scalar_multiply\n",
    "from typing import List\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z: float) -> float:\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "assert 0 < logit(-5.4) < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point_logit(x: Vector, theta: Vector) -> float:\n",
    "    return logit(predict_point(x, theta))\n",
    "\n",
    "def error_point(x: Vector, y: float, theta: Vector) -> float:\n",
    "    \"\"\" Calculates the prediction error at point x\"\"\"\n",
    "    return predict_point_logit(x, theta) - y\n",
    "    \n",
    "def gradient(x: List[Vector], y: float, theta: Vector) -> List:\n",
    "    \"\"\"Calculates the gradient update for 1 point for theta \"\"\"\n",
    "    return scalar_multiply(error_point(x, y, theta), x)\n",
    "\n",
    "def evaluate_gradient(data: List[List[Vector]], theta: Vector) -> float:\n",
    "    \"\"\"evaluates gradient for provided data structure \n",
    "    [[x_0, x_1,.., x_i],[x_0, x_1,.., x_i]] -> [y_1,y_2]\"\"\"\n",
    "    return vector_mean([gradient(x, y, theta) for x, y in data]) \n",
    "    \n",
    "def gradient_step(gradient: Vector, alpha: float) -> Vector:\n",
    "    \"\"\" takes one gradient step\"\"\"\n",
    "    return scalar_multiply(alpha, gradient)\n",
    "\n",
    "def theta_update(theta: Vector, gradient: Vector , alpha: float) -> Vector:\n",
    "    return add(theta, gradient_step(gradient, -alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _point_loss_function_logistic(y_hat:float, y: float) -> float:\n",
    "    \"\"\"for one point calculate the loss\"\"\"\n",
    "    return y * math.log(y_hat) + (1 - y) * (math.log(1 - y_hat))\n",
    "\n",
    "def loss_function_logistic(y_hats: Vector, ys: Vector) -> float:\n",
    "    \"\"\"for a pair of y_hat and y vectors calculate  the loss value\"\"\"\n",
    "    assert len(y_hat) == len(y)\n",
    "    m = len(y_hat)\n",
    "    return (-1/ m ) * sum( _point_loss_function_logistic(y_hat, y)\n",
    "                         for y_hat, y in zip(y_hats, ys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [vector[:-1] for vector in data]\n",
    "ys = [vector[-1] for vector in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 1.38807019, 1.850220317], 0], [[1, 5.332441248, 2.088626775], 1]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy = [[x, y] for x, y in zip(add_intercept_vectors(xs), ys)]\n",
    "data_copy\n",
    "chunk = list(chunks(data_copy, 2))[-1]\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2054491178999998, 7.0488227062105535, 4.321580305925604]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = evaluate_gradient(chunk, [0.4, 0.1, 1])\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-94452ee4f6ec>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-65-94452ee4f6ec>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    alpha: float,  epochs: int = 1000,\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def negative_log_fit(xs: List[float] , ys: List[float], \n",
    "                       alpha: float,  epochs: int = 1000, \n",
    "                       batch_size: int = 10) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the theta that minimizes the loss function\n",
    "    \"\"\"      \n",
    "    n = len(xs[0])                              # number of features\n",
    "    theta = [random.random() for _ in range(n)] # initial random guess for theta\n",
    "    m = len(xs)                                 # number trainning examples\n",
    "    loss = []                                   # to plot the avg. loss per epoch\n",
    "    \n",
    "    data = [[x, y] for x, y in zip(xs, ys)]     # zip data structure \n",
    "    for epoch in range(epochs):\n",
    "        chunks_loss = []                                # for avg. loss\n",
    "        for chunk in chunks(data, batch_size):\n",
    "            grad = evaluate_gradient(chunk, theta)      # evaluate the gradient    \n",
    "            theta = theta_update(theta, grad, alpha)    # update theta \n",
    "            y_hats = [predict_point_logit(x, theta) for x in xs]\n",
    "            loss_per_chunk = loss_function_logistic(y_hats, ys)  # loss per chunk iter\n",
    "            chunks_loss.append(loss_per_chunk)                          \n",
    "        average_epoch_loss = sum(chunks_loss) / len(chunks_loss)       \n",
    "        loss.append((epoch, average_epoch_loss))                # plot the avg per epoch\n",
    "    plot_loss(loss)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = add_intercept_vectors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3.396561688, 4.400293529, 0], [1, 6.922596716, 1.77106367, 1]]\n",
      "[[1, 1.38807019, 1.850220317, 0], [1, 8.675418651, -0.242068655, 1]]\n",
      "[[1, 5.332441248, 2.088626775, 1], [1, 7.627531214, 2.759262235, 1]]\n",
      "[[1, 2.7810836, 2.550537003, 0], [1, 3.06407232, 3.005305973, 0]]\n",
      "[[1, 7.673756466, 3.508563011, 1], [1, 1.465489372, 2.362125076, 0]]\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks(data_copy, 2):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-e568e84e0d68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnegative_log_theta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-a2e49b603a99>\u001b[0m in \u001b[0;36mnegative_log_theta\u001b[1;34m(data, alpha, epochs, batch_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mchunks_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m                                \u001b[1;31m# for avg. loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# evaluate the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# update theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-e2aac3b60b36>\u001b[0m in \u001b[0;36mevaluate_gradient\u001b[1;34m(data, theta)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"evaluates gradient for provided data structure \n\u001b[0;32m     14\u001b[0m     [[x_0, x_1,.., x_i],[x_0, x_1,.., x_i]] -> [y_1,y_2]\"\"\"\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvector_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-e2aac3b60b36>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"evaluates gradient for provided data structure \n\u001b[0;32m     14\u001b[0m     [[x_0, x_1,.., x_i],[x_0, x_1,.., x_i]] -> [y_1,y_2]\"\"\"\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvector_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "negative_log_theta(data, 0.01, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
