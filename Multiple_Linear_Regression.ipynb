{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b1a872-059a-4c6a-ace2-efbe29391094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scratch.multiple_regression import inputs\n",
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"\n",
    "    assumes that the first element of is 1\n",
    "    \"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x: Vector, y:float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "def squared_error(x: Vector, y:float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2\n",
    "\n",
    "x = [1,2,3]\n",
    "y = 30\n",
    "beta = [4,4,4]\n",
    "\n",
    "assert predict(x, beta) == 24\n",
    "assert error(x, y, beta) == -6\n",
    "assert squared_error(x, y, beta) == 36\n",
    "\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    \"\"\"\n",
    "    Partial derivative of eaxh x point\n",
    "    \"\"\"\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i\n",
    "            for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x, y, beta) == [-12, -24, -36]\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "\n",
    "def least_squares_fit(xs: list[Vector],\n",
    "                      ys: list[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Finds the beta that minimises the sum of squared errors\n",
    "    assuming the model y = dot(x, beta)\n",
    "    \"\"\"\n",
    "    # start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    \n",
    "    for _ in tqdm.trange(num_steps, desc='least squares fit'):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "            \n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            \n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "    return guess\n",
    "\n",
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: list[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, beta) ** 2\n",
    "                                for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)\n",
    "\n",
    "from typing import TypeVar, Callable\n",
    "\n",
    "X = TypeVar('X')\n",
    "Stat = TypeVar('Stat')\n",
    "\n",
    "\n",
    "# using bootstrap to calculate the coeficients\n",
    "\n",
    "def bootstrap_sample(data: list[X]) -> list[X]:\n",
    "    \"\"\"\n",
    "    Randomly samples len(data) elements with replacement\n",
    "    \"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data: list[x], \n",
    "                        stats_fn: Callable[list[X], Stat],\n",
    "                        num_samples: int) ->list[Stat]:\n",
    "    \"\"\"\n",
    "    Evalates stats_fn on num_samples bootstrap samples from data\n",
    "    \"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data))\n",
    "            for _ in range(num_samples)]\n",
    "\n",
    "\n",
    "# example 101 points all cose to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(100)]\n",
    "\n",
    "# 101 poibts, 50 close to 0, 50 near 200\n",
    "\n",
    "far_from_100 = ([99.5 + random.random()] +\n",
    "                [random.random() for _ in range(50)] +\n",
    "                [200 + random.random() for _ in range(50)]\n",
    "               )\n",
    "\n",
    "\n",
    "\n",
    "from scratch.statistics import standard_deviation, median\n",
    "\n",
    "medians_close = bootstrap_statistic(close_to_100, median, 100)\n",
    "medians_far = bootstrap_statistic(far_from_100, median, 100)\n",
    "\n",
    "\n",
    "assert standard_deviation(medians_close) < 1\n",
    "assert standard_deviation(medians_far) > 90\n",
    "\n",
    "# estimate the betas with bootstrap\n",
    "# then estimate their standard errors\n",
    "\n",
    "def estimate_sample_beta(pairs:list[tuple[Vector, float]]):\n",
    "    x_sample = [x for x, _ in pairs]\n",
    "    y_sample = [y for _, y in pairs]\n",
    "    \n",
    "    beta = least_squares_fit(xs=x_sample, ys=y_sample, \n",
    "                            num_steps=5000, batch_size=25)\n",
    "    print(\"bootstrap sample\", beta)\n",
    "    return beta\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "from scratch.statistics import daily_minutes_good\n",
    "\n",
    "# bootstrap_betas = bootstrap_statistic(list(zip(inputs, daily_minutes_good)),\n",
    "#                                       estimate_sample_beta,\n",
    "#                                       100)\n",
    "\n",
    "# bootstrap_standard_errors = [\n",
    "#     standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "#     for i in range(4)\n",
    "# ]\n",
    "# print(bootstrap_standard_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04da9c2-422c-48e5-9bb3-4b6902edd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.probability import normal_cdf\n",
    "\n",
    "\n",
    "#in order to test does our Beta_j is eaual to 0\n",
    "# we can calculate \n",
    "def p_value(beta_hat_j: float, sigma_hat_j: float) -> float:\n",
    "    if beta_hat_j > 0:\n",
    "        # if the coefficient is postitive, we need to compute twice\n",
    "        # the probability of seeing an even larger value\n",
    "        return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        # otherwise twice the probability of seeing a *smaller* value\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "\n",
    "assert p_value(30.58, 1.27)   < 0.001  # constant term\n",
    "assert p_value(0.972, 0.103)  < 0.001  # num_friends\n",
    "assert p_value(-1.865, 0.155) < 0.001  # work_hours\n",
    "assert p_value(0.923, 1.249)  > 0.4    # phd\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d7d20-935b-470a-af07-58a348e80936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|█| 5000/5000 [00:06<00:00, 801.70i\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "beta = least_squares_fit(inputs, daily_minutes_good, \n",
    "                         learning_rate, num_steps=5000, \n",
    "                         batch_size=25)\n",
    "assert 30.50 < beta[0] < 30.70  # constant\n",
    "assert  0.96 < beta[1] <  1.00  # num friends\n",
    "assert -1.89 < beta[2] < -1.85  # work hours per day\n",
    "assert  0.91 < beta[3] <  0.94  # has PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4332803-3b93-4399-bbc0-52840934de64",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a1dc35-1418-488f-83c9-339b2e5f9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is an approach in which we add to the error term a penalty \n",
    "# that gets larger as Beta gets larger. When then inimize the comined error and penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0056ee54-5ee1-4080-9297-c73055ca7b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares ridge fit: 100%|█| 5000/5000 [00:07<00:00, 6\n",
      "least squares ridge fit: 100%|█| 5000/5000 [00:07<00:00, 6\n",
      "least squares ridge fit: 100%|█| 5000/5000 [00:07<00:00, 6\n"
     ]
    }
   ],
   "source": [
    "def ridge_penalty(beta:Vector, alpha:float ) -> float:\n",
    "    \"\"\"\n",
    "    Alpha used as lambda\n",
    "    \"\"\"\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "\n",
    "def squared_error_ridge(x: Vector, \n",
    "                        y: float,\n",
    "                        beta: Vector,\n",
    "                        alpha: float) -> float:\n",
    "    \"\"\"\n",
    "    Estimated error plus ridge penalty on beta\n",
    "    \"\"\"\n",
    "    return error(x, y, beta) ** 2 + ridge_penalty_penalty(beta, alpha)\n",
    "\n",
    "# we can then plug this into gradient descent in the usual way\n",
    "\n",
    "from scratch.linear_algebra import add\n",
    "\n",
    "def ridge_penalty_gradient(beta: Vector, alpha:float) -> Vector:\n",
    "    \"\"\"\n",
    "    Gradient of just the ridge penalty\n",
    "    \"\"\"\n",
    "    return [0.] + [2 * alpha * beta_j\n",
    "                   for beta_j in beta[1:]]\n",
    "\n",
    "def sqerror_ridge_gradient(x: Vector,\n",
    "                           y: float,\n",
    "                           beta: Vector,\n",
    "                           alpha: float) -> Vector:\n",
    "    \"\"\"\n",
    "    the gradient corresponding to the ith squared errro\n",
    "    term including the ridge penalty\n",
    "    \"\"\"\n",
    "    return add(sqerror_gradient(x, y, beta),\n",
    "               ridge_penalty_gradient(beta, alpha))\n",
    "\n",
    "# note usually data is rescalaed prior to using a regularization technique\n",
    "    \n",
    "def least_squares_fit_ridge(xs: list[Vector],\n",
    "                      ys: list[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      alpha:float =0, \n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Finds the beta that minimises the sum of squared errors\n",
    "    assuming the model y = dot(x, beta)\n",
    "    \"\"\"\n",
    "    # start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    \n",
    "    for _ in tqdm.trange(num_steps, desc='least squares ridge fit'):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "            \n",
    "            gradient = vector_mean([sqerror_ridge_gradient(x, y, guess, alpha)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            \n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "    return guess \n",
    "\n",
    "random.seed(0)\n",
    "beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, \n",
    "                                 alpha=0, num_steps=5000,\n",
    "                                 batch_size=25)\n",
    "\n",
    "assert 5 < dot(beta_0[1:], beta_0[1: ]) < 6\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0) < 0.69\n",
    "\n",
    "# as we increase alpha the goodness of fit gets worst\n",
    "# but the size of beta gets smaller\n",
    "\n",
    "beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, \n",
    "                                 alpha=0.1, num_steps=5000,\n",
    "                                 batch_size=25)\n",
    "\n",
    "assert 4 < dot(beta_0_1[1:], beta_0_1[1: ]) < 5\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0_1) < 0.69\n",
    "\n",
    "beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good, \n",
    "                                 alpha=10, num_steps=5000,\n",
    "                                 batch_size=25)\n",
    "# phd coef was proven to be the least relevant\n",
    "# now reduced to close to 0\n",
    "assert 1 < dot(beta_10[1:], beta_10[1: ]) < 2\n",
    "assert 0.5 < multiple_r_squared(inputs, daily_minutes_good, beta_10) < 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97364bf-9516-4657-9052-6175dcd4eac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.514795945185586, 0.9748274277323267, -1.8506912934343662, 0.91407780744768]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91fd1bdf-0d42-467e-9cd6-94ba858bd404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.80152599845916, 0.9507225777158704, -1.833142990416332, 0.5384447644638315]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0174c6bf-3d40-414d-be46-d52cde0cbd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28.30708308025664,\n",
       " 0.6726275942984854,\n",
       " -0.9045499907700505,\n",
       " -0.0052131931011540865]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b9d59-cda4-4200-a704-9d61ba1d8db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
