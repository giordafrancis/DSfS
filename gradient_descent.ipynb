{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6c6a56-5c0b-4e59-9b1a-1e8e52287c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import Vector, dot, distance, add, scalar_multiply, vector_mean\n",
    "from typing import Callable, TypeVar, Iterator\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "def sum_of_squares(v:Vector) -> float:\n",
    "    \"\"\"\n",
    "    Compute the sum of squared elements in v\n",
    "    \"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c085e7-45a5-49c5-b1ee-cf1958d19237",
   "metadata": {},
   "source": [
    "**Gradient** -> gives you the imput direction which the function (sum of squares ie) most quickly increases. \n",
    "\n",
    "Similarly, you can try to minimise a function by taking small steps in the opposite direction. \n",
    "\n",
    "**Note** -> if a function has a global minimum, this procedure is likely to find it. Otherwise it's possible it will be \"stuck\" in a local minimum and run forever..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2efda8-e9ed-4562-8a3d-b32a616d73a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def square(x: float) -> float:\n",
    "    return x * x\n",
    "\n",
    "def derivative(x: float) -> float:\n",
    "    return 2 * x\n",
    "\n",
    "xs = range(-10, 11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.0001)\n",
    "             for x in xs]\n",
    "\n",
    "plt.title('Actual Derivatives vs Estimates')\n",
    "plt.plot(xs, actuals, 'rx', label='Actual')\n",
    "plt.plot(xs, estimates, 'b+', label='Estimate')\n",
    "plt.legend(loc=9);\n",
    "\n",
    "# we can estimate gradientes by evaluating difference quotient\n",
    "# for very small h\n",
    "# results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca7aff0-d73d-4d04-97f0-b2a2a3eaa6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when f is a function for several variables, it has multiple partial derivatives\n",
    "# each indicating how f changes when we make small changes in just one of the input variables\n",
    "\n",
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                       v: Vector,\n",
    "                       i: int,\n",
    "                       h: float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the i-th partial difference quotint of f at v\n",
    "    \"\"\"\n",
    "    w = [v_j + (h if j == i else 0) # add h to just ith element of v\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba0c291-a60c-4474-9b00-1db4b00830f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[13.208012514257312, -11.742984257911413, 5.865762683672816]\n",
      "[6.471926131986083, -5.754062286376592, 2.87422371499968]\n",
      "25\n",
      "[7.970569702832574, -7.086476821998351, 3.539780892849753]\n",
      "[3.905579154387961, -3.4723736427791922, 1.7344926374963792]\n",
      "50\n",
      "[4.809957691903722, -4.276438820471651, 2.136132919980079]\n",
      "[2.3568792690328237, -2.0954550220311092, 1.0467051307902389]\n",
      "75\n",
      "[2.9026398187926064, -2.58067999721191, 1.2890808753275844]\n",
      "[1.4222935112083772, -1.2645331986338357, 0.6316496289105163]\n",
      "100\n",
      "[1.751640753893146, -1.5573493571632895, 0.7779148420927046]\n",
      "[0.8583039694076415, -0.7631011850100119, 0.38117827262542525]\n",
      "125\n",
      "[1.0570534142178305, -0.939805408991883, 0.4694441699744672]\n",
      "[0.5179561729667369, -0.4605046504060227, 0.23002764328748893]\n",
      "150\n",
      "[0.6378944529728233, -0.5671394171820322, 0.2832929991799204]\n",
      "[0.3125682819566834, -0.27789831441919577, 0.138813569598161]\n",
      "175\n",
      "[0.38494680369070206, -0.3422486351367161, 0.17095733319836393]\n",
      "[0.188623933808444, -0.1677018312169909, 0.08376909326719832]\n",
      "200\n",
      "[0.2323018188684596, -0.20653498011997468, 0.10316672088227154]\n",
      "[0.11382789124554521, -0.10120214025878758, 0.05055169323231305]\n",
      "225\n",
      "[0.1401859543506013, -0.1246365759679904, 0.062257477339394886]\n",
      "[0.06869111763179464, -0.061071922224315296, 0.030506163896303493]\n",
      "250\n",
      "[0.08459727906098238, -0.07521377763709029, 0.037570191739333736]\n",
      "[0.04145266673988137, -0.03685475104217424, 0.01840939395227353]\n",
      "275\n",
      "[0.05105147414856566, -0.04538886199741673, 0.022672285605718375]\n",
      "[0.025015222332797172, -0.022240542378734195, 0.011109419946802004]\n",
      "300\n",
      "[0.030807764051878513, -0.02739057734024272, 0.013681924706524814]\n",
      "[0.01509580438542047, -0.013421382896718933, 0.006704143106197159]\n",
      "325\n",
      "[0.018591399008658888, -0.016529247353117546, 0.00825655899587821]\n",
      "[0.009109785514242855, -0.008099331203027598, 0.004045713907980323]\n",
      "350\n",
      "[0.011219253578971987, -0.009974817787397627, 0.004982542143351165]\n",
      "[0.005497434253696274, -0.004887660715824837, 0.0024414456502420707]\n",
      "375\n",
      "[0.006770423829355251, -0.0060194507206656435, 0.003006788448149377]\n",
      "[0.0033175076763840732, -0.0029495308531261653, 0.0014733263395931946]\n",
      "400\n",
      "[0.004085711986670471, -0.00363252620256388, 0.0018144907783647721]\n",
      "[0.0020019988734685306, -0.001779937839256301, 0.0008891004813987383]\n",
      "425\n",
      "[0.0024655830799904955, -0.0021921014432449743, 0.0010949811872521978]\n",
      "[0.0012081357091953428, -0.0010741297071900375, 0.0005365407817535769]\n",
      "450\n",
      "[0.0014878924271138842, -0.0013228559050957583, 0.000660782526278123]\n",
      "[0.0007290672892858032, -0.0006481993934969216, 0.0003237834378762803]\n",
      "475\n",
      "[0.0008978906014683471, -0.0007982968813050297, 0.00039875894866304426]\n",
      "[0.0004399663947194901, -0.00039116547183946454, 0.0001953918848448917]\n",
      "500\n",
      "[0.0005418453091861074, -0.00048174401176007603, 0.00024063696120186107]\n",
      "[0.00026550420150119264, -0.00023605456576243725, 0.00011791211098891193]\n",
      "525\n",
      "[0.00032698453308995727, -0.00029071551987939597, 0.000145215918766496]\n",
      "[0.00016022242121407906, -0.00014245060474090402, 7.115580019558303e-05]\n",
      "550\n",
      "[0.00019732363290300985, -0.00017543656264655118, 8.763268517801776e-05]\n",
      "[9.668858012247483e-05, -8.596391569681007e-05, 4.2940015737228705e-05]\n",
      "575\n",
      "[0.00011907785280880507, -0.00010586977787083954, 5.288323468075162e-05]\n",
      "[5.834814787631448e-05, -5.187619115671138e-05, 2.5912784993568295e-05]\n",
      "600\n",
      "[7.185928426791683e-05, -6.388867689457806e-05, 3.191316692645377e-05]\n",
      "[3.521104929127925e-05, -3.130545167834325e-05, 1.563745179396235e-05]\n",
      "625\n",
      "[4.3364543562843363e-05, -3.855456313811779e-05, 1.9258470655661252e-05]\n",
      "[2.124862634579325e-05, -1.8891735937677717e-05, 9.436650621274014e-06]\n",
      "650\n",
      "[2.616897256313646e-05, -2.3266319025887685e-05, 1.1621807790173435e-05]\n",
      "[1.2822796555936866e-05, -1.1400496322684966e-05, 5.694685817184983e-06]\n",
      "675\n",
      "[1.5792051956404515e-05, -1.4040402923906926e-05, 7.013351097639292e-06]\n",
      "[7.738105458638212e-06, -6.879797432714393e-06, 3.436542037843253e-06]\n",
      "700\n",
      "[9.52994636652595e-06, -8.472887956462329e-06, 4.232310024981427e-06]\n",
      "[4.669673719597715e-06, -4.151715098666541e-06, 2.0738319122408992e-06]\n",
      "725\n",
      "[5.750986508882962e-06, -5.11308904109341e-06, 2.554049825565934e-06]\n",
      "[2.8179833893526513e-06, -2.5054136301357708e-06, 1.2514844145273077e-06]\n",
      "750\n",
      "[3.470517519545137e-06, -3.0855688965188733e-06, 1.5412789878269846e-06]\n",
      "[1.700553584577117e-06, -1.511928759294248e-06, 7.552267040352225e-07]\n",
      "775\n",
      "[2.0943349171252338e-06, -1.8620320003519305e-06, 9.301075079029032e-07]\n",
      "[1.0262241093913645e-06, -9.123956801724459e-07, 4.5575267887242256e-07]\n",
      "800\n",
      "[1.2638572548294873e-06, -1.1236706379320363e-06, 5.612870759219488e-07]\n",
      "[6.192900548664488e-07, -5.505986125866978e-07, 2.750306672017549e-07]\n",
      "825\n",
      "[7.626932767647748e-07, -6.780955978801371e-07, 3.38716953599626e-07]\n",
      "[3.7371970561473964e-07, -3.322668429612672e-07, 1.6597130726381675e-07]\n",
      "850\n",
      "[4.602584921670361e-07, -4.092067767389965e-07, 2.0440373487552957e-07]\n",
      "[2.255266611618477e-07, -2.0051132060210827e-07, 1.0015783008900949e-07]\n",
      "875\n",
      "[2.7774976660402296e-07, -2.4694185694849195e-07, 1.2335044463245875e-07]\n",
      "[1.3609738563597126e-07, -1.2100150990476105e-07, 6.044171786990478e-08]\n",
      "900\n",
      "[1.6761218784984848e-07, -1.4902070097452072e-07, 7.443764273823349e-08]\n",
      "[8.212997204642576e-08, -7.302014347751515e-08, 3.647444494173441e-08]\n",
      "925\n",
      "[1.0114804364845852e-07, -8.992873704505094e-08, 4.492049196040614e-08]\n",
      "[4.956254138774468e-08, -4.406508115207496e-08, 2.2011041060599008e-08]\n",
      "950\n",
      "[6.103927682798109e-08, -5.426882100024915e-08, 2.7107932542421594e-08]\n",
      "[2.9909245645710734e-08, -2.659172229012208e-08, 1.328288694578658e-08]\n",
      "975\n",
      "[3.6835050696896895e-08, -3.274931940033466e-08, 1.635868118657702e-08]\n",
      "[1.804917484147948e-08, -1.6047166506163984e-08, 8.01575378142274e-09]\n"
     ]
    }
   ],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector,\n",
    "                  step_size:float) -> Vector:\n",
    "    \"\"\"\n",
    "    Moves 'step_size' in the 'gradient' direction from v\n",
    "    \"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    \n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector)->Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v) # compute gradient at v\n",
    "    v = gradient_step(v, grad, -0.01) # take a negative grad. step\n",
    "    if epoch % 25 == 0:\n",
    "        print(epoch)\n",
    "        print(grad)\n",
    "        print(v)\n",
    "        \n",
    "# v will ve very close to 0\n",
    "assert distance(v, [0,0,0]) < 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7390b848-2a07-4115-a86c-cf2df4e5905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example -50 to 40 xs, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
    "\n",
    "# we'll use the gradient descent to find the slope and intercept that minimizes the avergae squared err\n",
    "\n",
    "def linear_gradient(x: float, y: float, theta:Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept # model predic.\n",
    "    error = (predicted - y)\n",
    "    squared_error = error ** 2 # we'll minimise sq. error\n",
    "    grad = [2 * error * x, 2 * error] # using its gradient\n",
    "    return grad\n",
    "\n",
    "# Now this computation is for single data point\n",
    "# for the whole data we'll look into th emean squared error\n",
    "# the gradient of the mean sq err is just the mean of the individal gradients\n",
    "\n",
    "# we will:\n",
    "# 1 - start with random val for theta\n",
    "# 2 - compute the mean of the gradients\n",
    "# 3 - adjust the theta in that direction\n",
    "# 4 - repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52729534-f19f-4bd4-a192-3acd47467b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.986310602026621, -0.3771089646381105]\n",
      "0\n",
      "[-31690.443117456973, 8.259471468697223]\n",
      "[32.67675371948359, -0.3853684361068077]\n",
      "1000\n",
      "[-0.0008744960356102638, -1.4560364257889171]\n",
      "[19.99956349513274, 4.2732191338496675]\n",
      "2000\n",
      "[-0.00011818449325403436, -0.1967772501871452]\n",
      "[19.99994100818774, 4.901778597158228]\n",
      "3000\n",
      "[-1.5972139063791246e-05, -0.02659362465481296]\n",
      "[19.99999202750261, 4.986725787062468]\n",
      "4000\n",
      "[-2.1585679269264803e-06, -0.003594017456885883]\n",
      "[19.99999892255023, 4.998206045484863]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "print(theta)\n",
    "learning_rate = 0.001\n",
    "for epoch in range(5_000):\n",
    "    # Compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x, y, theta)\n",
    "                        for x, y in inputs])\n",
    "    theta = gradient_step(theta, grad, - learning_rate)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch)\n",
    "        print(grad)\n",
    "        print(theta)\n",
    "        \n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1\n",
    "assert 4.9 < intercept < 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cf0be-f203-4796-a5f6-5ef55776550d",
   "metadata": {},
   "source": [
    "**Minibatch and Stochastic Gradient descent**\n",
    "\n",
    "A drawback of preceding approach is we have to evaluate the gradient in all points of x, for larger datasets this will be computationally expensive.\n",
    "\n",
    "An alternative is the `minibatch gradient descent`, where we compute the gradient and take a gradient step based on a minibatch sampled from a large dataset.\n",
    "\n",
    "Another variation is the `stochastic gradient descent`, in which you take the gradient steps based on **one** training example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "171d8903-87bb-484c-8e03-42d9668b0735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[(45, 905), (46, 925), (47, 945), (48, 965), (49, 985)]\n",
      "[2227.6327828835033, 47.37986201482486]\n",
      "[17.967184263800252, 19.486149951261044]\n",
      "200\n",
      "[(10, 205), (11, 225), (12, 245), (13, 265), (14, 285)]\n",
      "[0.14209704250652067, 0.011681801381769219]\n",
      "[20.000336759438817, 5.000082941113647]\n",
      "400\n",
      "[(-10, -195), (-9, -175), (-8, -155), (-7, -135), (-6, -115)]\n",
      "[-7.538410000051954e-06, 9.360331887364737e-07]\n",
      "[19.99999999500229, 5.0000003667916015]\n",
      "600\n",
      "[(-10, -195), (-9, -175), (-8, -155), (-7, -135), (-6, -115)]\n",
      "[5.655317636410473e-09, -7.091273346304661e-10]\n",
      "[19.99999999998992, 4.999999999610754]\n",
      "800\n",
      "[(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915)]\n",
      "[0.0, 0.0]\n",
      "[20.0, 4.999999999999991]\n"
     ]
    }
   ],
   "source": [
    "T = TypeVar('T')\n",
    "\n",
    "def minibatches(dataset: list[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True)-> Iterator[list[T]]:\n",
    "    \"\"\"\n",
    "    Generates 'batch_size' -seized inibatches from the dataset\n",
    "    \"\"\"\n",
    "    batch_starts = [start \n",
    "                    for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    if shuffle: random.shuffle(batch_starts) # shuffle batches\n",
    "    \n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start : end]\n",
    "\n",
    "                    \n",
    "\n",
    "# solve same problem again using minibatches\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=5):\n",
    "    \n",
    "        grad = vector_mean([linear_gradient(x, y, theta)\n",
    "                           for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch)\n",
    "        print(batch)\n",
    "        print(grad)\n",
    "        print(theta)\n",
    "        \n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1\n",
    "assert 4.9 < intercept < 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61577566-a3fd-4a70-a827-e779401a8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4823362510054805, -0.9672518471963591]\n",
      "0\n",
      "[2.4373700930193536, 0.04974224679631334]\n",
      "[20.12252912749519, -1.098537000670952]\n",
      "10\n",
      "[1.5726647351777956, 0.03209519867709787]\n",
      "[20.079030172191892, 1.0664764947134666]\n",
      "20\n",
      "[1.0143605222274346, 0.020701235147498664]\n",
      "[20.05097403514134, 2.4628985288237164]\n",
      "30\n",
      "[0.6542473906565647, 0.013351987564419687]\n",
      "[20.03287798573138, 3.3635832188149406]\n",
      "40\n",
      "[0.42199870573358567, 0.008612218484358891]\n",
      "[20.021206113647334, 3.94451999172344]\n",
      "50\n",
      "[0.27216895482365544, 0.0055544684657888865]\n",
      "[20.01367784327181, 4.319221080659415]\n",
      "60\n",
      "[0.1755576547104738, 0.0035828092798055877]\n",
      "[20.008822127159807, 4.560901265919266]\n",
      "70\n",
      "[0.11324670474891718, 0.00231115723977382]\n",
      "[20.005690214257267, 4.716783680323819]\n",
      "80\n",
      "[0.07304775000579866, 0.0014907704082816053]\n",
      "[20.003670154171967, 4.8173270002570305]\n",
      "90\n",
      "[0.047101823824277744, 0.0009612617106995458]\n",
      "[20.0023672404416, 4.882176898587888]\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "print(theta)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch)\n",
    "        print(grad)\n",
    "        print(theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1\n",
    "assert 4.9 < intercept < 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da609f-0a55-49d4-9126-dcf0e1506e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
